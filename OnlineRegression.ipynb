{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b36ade28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of online linear regression.\n",
    "\n",
    "# Python 3.8.10\n",
    "import numpy as np # Test in version 1.21.1.\n",
    "import random\n",
    "from sklearn import datasets, linear_model # sklearn v. 0.24.2\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import linalg # Scipy 1.7.0\n",
    "import sympy # 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67320674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416.15301008042337,\n",
       " 82.8002593290743,\n",
       " 1010,\n",
       " 0.49901136647134914,\n",
       " 5.025986795844447,\n",
       " 0.506835402394537,\n",
       " 3.046359406572799)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Online linear regression with a single feature is demonstrated here:\n",
    "# https://stackoverflow.com/questions/52070293/efficient-online-linear-regression-algorithm-in-python\n",
    "# Following is a direct lifting of the code in the first answer\n",
    "# Note: if all values of new_x in the first array are the same, the result is a divide-by-zero error.\n",
    "\n",
    "def lr(x_avg,y_avg,Sxy,Sx,n,new_x,new_y):\n",
    "    \"\"\"\n",
    "    x_avg: average of previous x, if no previous sample, set to 0\n",
    "    y_avg: average of previous y, if no previous sample, set to 0\n",
    "    Sxy: covariance of previous x and y, if no previous sample, set to 0\n",
    "    Sx: variance of previous x, if no previous sample, set to 0\n",
    "    n: number of previous samples\n",
    "    new_x: new incoming 1-D numpy array x\n",
    "    new_y: new incoming 1-D numpy array x\n",
    "    \"\"\"\n",
    "    new_n = n + len(new_x)\n",
    "\n",
    "    new_x_avg = (x_avg*n + np.sum(new_x))/new_n\n",
    "    new_y_avg = (y_avg*n + np.sum(new_y))/new_n\n",
    "\n",
    "    if n > 0:\n",
    "        x_star = (x_avg*np.sqrt(n) + new_x_avg*np.sqrt(new_n))/(np.sqrt(n)+np.sqrt(new_n))\n",
    "        y_star = (y_avg*np.sqrt(n) + new_y_avg*np.sqrt(new_n))/(np.sqrt(n)+np.sqrt(new_n))\n",
    "    elif n == 0:\n",
    "        x_star = new_x_avg\n",
    "        y_star = new_y_avg\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    new_Sx = Sx + np.sum((new_x-x_star)**2)\n",
    "    new_Sxy = Sxy + np.sum((new_x-x_star).reshape(-1) * (new_y-y_star).reshape(-1))\n",
    "\n",
    "    beta = new_Sxy/new_Sx\n",
    "    alpha = new_y_avg - beta * new_x_avg\n",
    "    return new_Sxy, new_Sx, new_n, alpha, beta, new_x_avg, new_y_avg\n",
    "\n",
    "# Example of online linear regression applied to 101 batches of random data.\n",
    "x_avg, y_avg, Sxy, Sx, n = 0,0,0,0,0\n",
    "random.seed(1234)\n",
    "X = np.array([random.random() for i in range(10)])\n",
    "y = np.array([random.random() + 5*X[i] for i in range(10)])\n",
    "\n",
    "X_total = X\n",
    "y_total = y\n",
    "\n",
    "Sxy, Sx, n, alpha, beta, x_avg, y_avg = lr(x_avg,y_avg,Sxy,Sx,n, X,y)\n",
    "\n",
    "for i in range(100):\n",
    "    X = np.array([random.random() for i in range(10)])\n",
    "    X_total = np.append(X_total, X)\n",
    "    y = np.array([random.random() + 5*X[i] for i in range(10)])\n",
    "    y_total = np.append(y_total, y)\n",
    "    Sxy, Sx, n, alpha, beta, x_avg, y_avg = lr(x_avg,y_avg,Sxy,Sx,n, X,y)\n",
    "    \n",
    "# Results. alpha and beta are, respectively, the intercept and coefficient of the regression.\n",
    "Sxy, Sx, n, alpha, beta, x_avg, y_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a195c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4990113664713478, array([5.0259868])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use scikit learn's linear model to validate the above algorithm\n",
    "# The intercept and coefficient should match the alpha and beta values, respectively, found above.\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_total.reshape(-1,1), y_total)\n",
    "[regr.intercept_,regr.coef_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb4a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c170182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online multidimensional regression:\n",
    "# The following is based on a formula for regression coefficient, given for example in the following:\n",
    "# https://stattrek.com/multiple-regression/regression-coefficients.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08929108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core of this notebook: implementation of online regression.\n",
    "# There are n training examples with p features each.\n",
    "# If results are calculated once and n>p, then total time is O(np^2 + p^3).\n",
    "# If results are calculated once for every training example, then time is O(np^3).\n",
    "# For complexity purposes, the sizes of data batches is irrelevant.\n",
    "\n",
    "def lr_multi(XX,Xy,X,y, calc_results=False):\n",
    "    # Time complexity analysis is noted. XX is a (p+1)X(p+1) matrix and Xy is a vector of p+1 numbers.\n",
    "    # Overall complexity is O(np^2).\n",
    "    XX = np.add(XX, np.matmul(X.transpose(),X))\n",
    "    Xy = np.add(Xy, np.matmul(X.transpose(),y))\n",
    "    if (calc_results):\n",
    "        # We are assuming O(p^3) complexity for row reduced echelon form, as floating point operations are employed.\n",
    "        # See also this discussion:\n",
    "        # https://cstheory.stackexchange.com/questions/3921/what-is-the-actual-time-complexity-of-gaussian-elimination\n",
    "        lin_ind_cols = sympy.Matrix(XX).T.rref()[1]\n",
    "        # O(p^2) space and time to reduce XX and Xy to linearly independent features\n",
    "        XX_reduced = [\n",
    "                [XX[i][j] for j in range(len(XX[0])) if j in lin_ind_cols]\n",
    "            for i in range(len(XX)) if i in lin_ind_cols]\n",
    "        Xy_reduced = [[Xy[i][0]] for i in range(len(XX)) if i in lin_ind_cols]\n",
    "        # XX_reduced is of size O(p).\n",
    "        # Matrix inversion takes O(p^3) time under Gauss-Jordan.\n",
    "        # See https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n",
    "        # Faster algorithms may be possible.\n",
    "        # Matrix multiplication is O(p^2) under naive approaches.\n",
    "        result = np.matmul( np.linalg.inv(XX_reduced), Xy_reduced )\n",
    "        # Rebuild the full results from the above results with only linear independent features.\n",
    "        # Should be O(p) time to do the following.\n",
    "        full_result = np.zeros((len(XX)))\n",
    "        result_pointer = 0\n",
    "        for i in range(len(XX)):\n",
    "            if i in lin_ind_cols:\n",
    "                full_result[i] = result[result_pointer]\n",
    "                result_pointer += 1\n",
    "        return XX, Xy, {\"intercept\":full_result[0], \"coefficients\":full_result[1:]}\n",
    "    else:\n",
    "        return XX, Xy, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0050d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intercept': 9.04545454545456, 'coefficients': array([-2.27272727,  1.85227273])}\n",
      "{'intercept': 9.045454545454543, 'coefficients': array([-2.27272727,  1.85227273])}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing\n",
    "def example():\n",
    "    y_base = np.array([10,11,12,7,7])\n",
    "    X_base = np.array([[1,2],[0,1],[3,5],[2,1],[3,3]])\n",
    "    y = y_base.reshape(1,len(y_base)).transpose()\n",
    "    X = np.concatenate(([[1]]*len(X_base),X_base), axis=1)\n",
    "    XX = np.zeros( ( len(X[0]) , len(X[0]) ) )\n",
    "    Xy = np.zeros( ( len(X[0]) , 1 ) )\n",
    "\n",
    "    # Split into 2\n",
    "    X1, X2, y1, y2 = X[:3], X[3:], y[:3], y[3:]\n",
    "    XX, Xy, _ = lr_multi(XX,Xy,X1,y1)\n",
    "    XX, Xy, results = lr_multi(XX,Xy,X2,y2, True)\n",
    "    print(results)\n",
    "\n",
    "    # Validate with scikit learn. Should match the intercept and coefficients found above.\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_base, y_base)\n",
    "    print({\"intercept\":regr.intercept_, \"coefficients\":regr.coef_})\n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d22aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intercept': 11.0, 'coefficients': array([-1.,  0.])}\n",
      "{'intercept': 11.0, 'coefficients': array([-0.2, -0.4])}\n"
     ]
    }
   ],
   "source": [
    "# Another example. The matrix XX in the algorithm is singular.\n",
    "\n",
    "def singular_example():\n",
    "    y_base = np.array([10,11,12,7,7])\n",
    "    # Note that the second column is twice the first, making it redundant for regression purposes.\n",
    "    X_base = np.array([[1,2],[2,4],[1,2],[3,6],[1,2]]) \n",
    "    y = y_base.reshape(1,len(y_base)).transpose()\n",
    "    X = np.concatenate(([[1]]*len(X_base),X_base), axis=1)\n",
    "    XX = np.zeros( ( len(X[0]) , len(X[0]) ) )\n",
    "    Xy = np.zeros( ( len(X[0]) , 1 ) )\n",
    "\n",
    "    X1,X2,y1,y2 = X[:2],X[2:],y[:2],y[2:]\n",
    "\n",
    "    XX, Xy, _ = lr_multi(XX,Xy,X1,y1)\n",
    "    XX, Xy, results = lr_multi(XX,Xy,X2,y2,True)\n",
    "    print(results)\n",
    "    # These results are not the same as the results given by scikit-learn's regression.\n",
    "    # In the event of the X^TX matrix being singular (linear dependency among features), there is not an unambiguous\n",
    "    # regression that minimizes error.\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_base, y_base)\n",
    "    print({\"intercept\":regr.intercept_, \"coefficients\":regr.coef_})\n",
    "singular_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07b79fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intercept': 0.42971638609834667, 'coefficients': array([-0.00111586,  0.02239917,  0.02781732, -0.01231627,  0.07486401,\n",
      "       -0.01070935,  0.0487162 , -0.01034687,  0.01303653,  0.00970912])}\n",
      "{'intercept': 0.4297163860983432, 'coefficients': array([-0.00111586,  0.02239917,  0.02781732, -0.01231627,  0.07486401,\n",
      "       -0.01070935,  0.0487162 , -0.01034687,  0.01303653,  0.00970912])}\n"
     ]
    }
   ],
   "source": [
    "def large_example():\n",
    "    # A larger example\n",
    "    y_base = np.random.rand(1000)\n",
    "    X_base = np.random.rand(1000,10)\n",
    "    \n",
    "    y = y_base.reshape(1,len(y_base)).transpose()\n",
    "    X = np.concatenate(([[1]]*len(X_base),X_base), axis=1)\n",
    "    XX = np.zeros( ( len(X[0]) , len(X[0]) ) )\n",
    "    Xy = np.zeros( ( len(X[0]) , 1 ) )\n",
    "    \n",
    "    breakpoints = [0,300,500,800,1000]\n",
    "    for i in range(len(breakpoints)-2):\n",
    "        XX, Xy, _ = lr_multi(XX,Xy,X[breakpoints[i]:breakpoints[i+1]], y[breakpoints[i]:breakpoints[i+1]])\n",
    "    _,_,results = lr_multi(XX,Xy,X[breakpoints[-2]:], y[breakpoints[-2]:],True)\n",
    "    print(results)\n",
    "    \n",
    "    # Validate with scikit-learn's linear regression.\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_base, y_base)\n",
    "    print({\"intercept\":regr.intercept_, \"coefficients\":regr.coef_})\n",
    "results = large_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b88a2104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intercept': 3.0, 'coefficients': array([0., 0.])}\n",
      "{'intercept': 3.0, 'coefficients': array([0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Example of a situation where all inputs are identical.\n",
    "# In that case, it is impossible to learn any coefficient weights, so the result is merely an intercept\n",
    "# that is the average of all responses.\n",
    "def degenerate_example():\n",
    "    y_base = np.array([1,2,3,4,5])\n",
    "    X_base = np.array([[1,1],[1,1],[1,1],[1,1],[1,1]])\n",
    "    \n",
    "    y = y_base.reshape(1,len(y_base)).transpose()\n",
    "    X = np.concatenate(([[1]]*len(X_base),X_base), axis=1)\n",
    "    XX = np.zeros( ( len(X[0]) , len(X[0]) ) )\n",
    "    Xy = np.zeros( ( len(X[0]) , 1 ) )\n",
    "    _,_,results = lr_multi(XX,Xy,X,y,True)\n",
    "    print(results)\n",
    "    \n",
    "    # Validate with scikit-learn's linear regression.\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_base, y_base)\n",
    "    print({\"intercept\":regr.intercept_, \"coefficients\":regr.coef_})\n",
    "    \n",
    "degenerate_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771b377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
